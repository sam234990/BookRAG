{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b472ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 888/888 [00:00<00:00, 6417.46 examples/s]\n",
      "Generating validation split: 100%|██████████| 281/281 [00:00<00:00, 5650.40 examples/s]\n",
      "Generating test split: 100%|██████████| 416/416 [00:00<00:00, 6046.01 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "qasper_dataset = load_dataset(\n",
    "    \"allenai/qasper\",\n",
    "    cache_dir=\"./hf_cache\",\n",
    "    trust_remote_code=True,\n",
    "\n",
    ")\n",
    "print(len(qasper_dataset['train']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eac03074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集加载完成！\n",
      "\n",
      "正在从训练集中随机抽取 200 行数据...\n",
      "成功抽取 200 行数据。\n",
      "\n",
      "这 200 行数据中总共包含 590 个问题。\n",
      "\n",
      "正在将数据转换为 Pandas DataFrame 并移除 'full_text' 字段...\n",
      "(200, 5)\n"
     ]
    }
   ],
   "source": [
    "train_set = qasper_dataset['train']\n",
    "print(\"数据集加载完成！\")\n",
    "\n",
    "# --- 2. 随机抽样 ---\n",
    "num_samples = 200\n",
    "# 使用 .shuffle() 来打乱数据集，然后用 .select() 抽取前200个\n",
    "# 设置 seed=42 可以确保每次随机抽样的结果都一样，方便复现\n",
    "print(f\"\\n正在从训练集中随机抽取 {num_samples} 行数据...\")\n",
    "sampled_dataset = train_set.shuffle(seed=42).select(range(num_samples))\n",
    "print(f\"成功抽取 {len(sampled_dataset)} 行数据。\")\n",
    "\n",
    "# --- 3. 统计问题总数 ---\n",
    "total_questions = 0\n",
    "# 遍历抽样后的数据集\n",
    "for example in sampled_dataset:\n",
    "    # 累加每个样本中 'qas'->'question' 列表的长度\n",
    "    total_questions += len(example['qas']['question'])\n",
    "\n",
    "print(f\"\\n这 {num_samples} 行数据中总共包含 {total_questions} 个问题。\")\n",
    "\n",
    "# --- 4. 转换为DataFrame并移除不需要的列 ---\n",
    "print(\"\\n正在将数据转换为 Pandas DataFrame 并移除 'full_text' 字段...\")\n",
    "# 使用 .remove_columns() 方法可以高效地移除列\n",
    "df = sampled_dataset.remove_columns(['full_text']).to_pandas()\n",
    "\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "936eb7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "已将 ID 列表保存到 ./qasper_id_list.txt，共 200 个 ID。\n"
     ]
    }
   ],
   "source": [
    "id_list = df['id'].tolist()\n",
    "\n",
    "save_id_path = \"./qasper_id_list.txt\"\n",
    "with open(save_id_path, \"w\") as f:\n",
    "    for _id in id_list:\n",
    "        f.write(f\"{_id}\\n\")\n",
    "print(f\"\\n已将 ID 列表保存到 {save_id_path}，共 {len(id_list)} 个 ID。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ebb23531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "展开后的DataFrame形状: (590, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>figures_and_tables</th>\n",
       "      <th>question</th>\n",
       "      <th>question_id</th>\n",
       "      <th>nlp_background</th>\n",
       "      <th>topic_background</th>\n",
       "      <th>paper_read</th>\n",
       "      <th>search_query</th>\n",
       "      <th>question_writer</th>\n",
       "      <th>answers_ori</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1909.08402</td>\n",
       "      <td>Enriching BERT with Knowledge Graph Embeddings...</td>\n",
       "      <td>In this paper, we focus on the classification ...</td>\n",
       "      <td>{'caption': ['Table 1: Availability of additio...</td>\n",
       "      <td>By how much do they outperform standard BERT?</td>\n",
       "      <td>f5cf8738e8d211095bb89350ed05ee7f9997eb19</td>\n",
       "      <td>five</td>\n",
       "      <td>familiar</td>\n",
       "      <td>no</td>\n",
       "      <td></td>\n",
       "      <td>5053f146237e8fc8859ed3984b5d3f02f39266b7</td>\n",
       "      <td>{'answer': [{'unanswerable': False, 'extractiv...</td>\n",
       "      <td>[{'unanswerable': False, 'extractive_spans': [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1909.08402</td>\n",
       "      <td>Enriching BERT with Knowledge Graph Embeddings...</td>\n",
       "      <td>In this paper, we focus on the classification ...</td>\n",
       "      <td>{'caption': ['Table 1: Availability of additio...</td>\n",
       "      <td>What dataset do they use?</td>\n",
       "      <td>bed527bcb0dd5424e69563fba4ae7e6ea1fca26a</td>\n",
       "      <td>five</td>\n",
       "      <td>familiar</td>\n",
       "      <td>no</td>\n",
       "      <td></td>\n",
       "      <td>5053f146237e8fc8859ed3984b5d3f02f39266b7</td>\n",
       "      <td>{'answer': [{'unanswerable': False, 'extractiv...</td>\n",
       "      <td>[{'unanswerable': False, 'extractive_spans': [...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                              title  \\\n",
       "0  1909.08402  Enriching BERT with Knowledge Graph Embeddings...   \n",
       "0  1909.08402  Enriching BERT with Knowledge Graph Embeddings...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  In this paper, we focus on the classification ...   \n",
       "0  In this paper, we focus on the classification ...   \n",
       "\n",
       "                                  figures_and_tables  \\\n",
       "0  {'caption': ['Table 1: Availability of additio...   \n",
       "0  {'caption': ['Table 1: Availability of additio...   \n",
       "\n",
       "                                        question  \\\n",
       "0  By how much do they outperform standard BERT?   \n",
       "0                      What dataset do they use?   \n",
       "\n",
       "                                question_id nlp_background topic_background  \\\n",
       "0  f5cf8738e8d211095bb89350ed05ee7f9997eb19           five         familiar   \n",
       "0  bed527bcb0dd5424e69563fba4ae7e6ea1fca26a           five         familiar   \n",
       "\n",
       "  paper_read search_query                           question_writer  \\\n",
       "0         no               5053f146237e8fc8859ed3984b5d3f02f39266b7   \n",
       "0         no               5053f146237e8fc8859ed3984b5d3f02f39266b7   \n",
       "\n",
       "                                         answers_ori  \\\n",
       "0  {'answer': [{'unanswerable': False, 'extractiv...   \n",
       "0  {'answer': [{'unanswerable': False, 'extractiv...   \n",
       "\n",
       "                                              answer  \n",
       "0  [{'unanswerable': False, 'extractive_spans': [...  \n",
       "0  [{'unanswerable': False, 'extractive_spans': [...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 假设 df 已经从之前的单元格中定义\n",
    "# 如果没有，请确保运行之前的单元格来定义 df\n",
    "\n",
    "expanded_rows = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    qas = row['qas']\n",
    "    questions = qas['question']\n",
    "    question_ids = qas['question_id']\n",
    "    nlp_bg = qas['nlp_background']\n",
    "    topic_bg = qas['topic_background']\n",
    "    paper_read = qas['paper_read']\n",
    "    search_query = qas['search_query']\n",
    "    question_writer = qas['question_writer']\n",
    "    answers = qas['answers']\n",
    "    \n",
    "    for i in range(len(questions)):\n",
    "        new_row = row.copy()\n",
    "        new_row['question'] = questions[i]\n",
    "        new_row['question_id'] = question_ids[i]\n",
    "        new_row['nlp_background'] = nlp_bg[i]\n",
    "        new_row['topic_background'] = topic_bg[i]\n",
    "        new_row['paper_read'] = paper_read[i]\n",
    "        new_row['search_query'] = search_query[i]\n",
    "        new_row['question_writer'] = question_writer[i]\n",
    "        new_row['answers_ori'] = answers[i]\n",
    "        expanded_rows.append(new_row)\n",
    "\n",
    "expanded_df = pd.DataFrame(expanded_rows)\n",
    "\n",
    "# 移除原来的 'qas' 列\n",
    "expanded_df = expanded_df.drop(columns=['qas'])\n",
    "\n",
    "expanded_df['answer'] = expanded_df['answers_ori'].apply(lambda x: x['answer'])\n",
    "\n",
    "print(f\"展开后的DataFrame形状: {expanded_df.shape}\")\n",
    "expanded_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40400050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(590, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>figures_and_tables</th>\n",
       "      <th>question</th>\n",
       "      <th>question_id</th>\n",
       "      <th>nlp_background</th>\n",
       "      <th>topic_background</th>\n",
       "      <th>paper_read</th>\n",
       "      <th>search_query</th>\n",
       "      <th>question_writer</th>\n",
       "      <th>answers_ori</th>\n",
       "      <th>answer</th>\n",
       "      <th>doc_uuid</th>\n",
       "      <th>doc_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1909.08402</td>\n",
       "      <td>Enriching BERT with Knowledge Graph Embeddings...</td>\n",
       "      <td>In this paper, we focus on the classification ...</td>\n",
       "      <td>{'caption': ['Table 1: Availability of additio...</td>\n",
       "      <td>By how much do they outperform standard BERT?</td>\n",
       "      <td>f5cf8738e8d211095bb89350ed05ee7f9997eb19</td>\n",
       "      <td>five</td>\n",
       "      <td>familiar</td>\n",
       "      <td>no</td>\n",
       "      <td></td>\n",
       "      <td>5053f146237e8fc8859ed3984b5d3f02f39266b7</td>\n",
       "      <td>{'answer': [{'unanswerable': False, 'extractiv...</td>\n",
       "      <td>[{'unanswerable': False, 'extractive_spans': [...</td>\n",
       "      <td>df4a3d95-a9f7-58ee-8584-48b8ff161556</td>\n",
       "      <td>/mnt/data/wangshu/mmrag/qasper/documents/1909....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1909.08402</td>\n",
       "      <td>Enriching BERT with Knowledge Graph Embeddings...</td>\n",
       "      <td>In this paper, we focus on the classification ...</td>\n",
       "      <td>{'caption': ['Table 1: Availability of additio...</td>\n",
       "      <td>What dataset do they use?</td>\n",
       "      <td>bed527bcb0dd5424e69563fba4ae7e6ea1fca26a</td>\n",
       "      <td>five</td>\n",
       "      <td>familiar</td>\n",
       "      <td>no</td>\n",
       "      <td></td>\n",
       "      <td>5053f146237e8fc8859ed3984b5d3f02f39266b7</td>\n",
       "      <td>{'answer': [{'unanswerable': False, 'extractiv...</td>\n",
       "      <td>[{'unanswerable': False, 'extractive_spans': [...</td>\n",
       "      <td>df4a3d95-a9f7-58ee-8584-48b8ff161556</td>\n",
       "      <td>/mnt/data/wangshu/mmrag/qasper/documents/1909....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                              title  \\\n",
       "0  1909.08402  Enriching BERT with Knowledge Graph Embeddings...   \n",
       "0  1909.08402  Enriching BERT with Knowledge Graph Embeddings...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  In this paper, we focus on the classification ...   \n",
       "0  In this paper, we focus on the classification ...   \n",
       "\n",
       "                                  figures_and_tables  \\\n",
       "0  {'caption': ['Table 1: Availability of additio...   \n",
       "0  {'caption': ['Table 1: Availability of additio...   \n",
       "\n",
       "                                        question  \\\n",
       "0  By how much do they outperform standard BERT?   \n",
       "0                      What dataset do they use?   \n",
       "\n",
       "                                question_id nlp_background topic_background  \\\n",
       "0  f5cf8738e8d211095bb89350ed05ee7f9997eb19           five         familiar   \n",
       "0  bed527bcb0dd5424e69563fba4ae7e6ea1fca26a           five         familiar   \n",
       "\n",
       "  paper_read search_query                           question_writer  \\\n",
       "0         no               5053f146237e8fc8859ed3984b5d3f02f39266b7   \n",
       "0         no               5053f146237e8fc8859ed3984b5d3f02f39266b7   \n",
       "\n",
       "                                         answers_ori  \\\n",
       "0  {'answer': [{'unanswerable': False, 'extractiv...   \n",
       "0  {'answer': [{'unanswerable': False, 'extractiv...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  [{'unanswerable': False, 'extractive_spans': [...   \n",
       "0  [{'unanswerable': False, 'extractive_spans': [...   \n",
       "\n",
       "                               doc_uuid  \\\n",
       "0  df4a3d95-a9f7-58ee-8584-48b8ff161556   \n",
       "0  df4a3d95-a9f7-58ee-8584-48b8ff161556   \n",
       "\n",
       "                                            doc_path  \n",
       "0  /mnt/data/wangshu/mmrag/qasper/documents/1909....  \n",
       "0  /mnt/data/wangshu/mmrag/qasper/documents/1909....  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import uuid\n",
    "import os\n",
    "pdf_path = \"xxxx/qasper/documents\"\n",
    "\n",
    "expanded_df['doc_uuid'] = expanded_df['id'].apply(lambda x: str(uuid.uuid5(uuid.NAMESPACE_DNS, x)))\n",
    "expanded_df['doc_path'] = expanded_df.apply(lambda row: os.path.join(pdf_path, row[\"id\"] + \".pdf\"), axis=1)\n",
    "print(expanded_df.shape)\n",
    "expanded_df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e95177c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(590, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question_id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>doc_uuid</th>\n",
       "      <th>doc_path</th>\n",
       "      <th>abstract</th>\n",
       "      <th>answers_ori</th>\n",
       "      <th>figures_and_tables</th>\n",
       "      <th>nlp_background</th>\n",
       "      <th>paper_read</th>\n",
       "      <th>question_writer</th>\n",
       "      <th>search_query</th>\n",
       "      <th>title</th>\n",
       "      <th>topic_background</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1909.08402</td>\n",
       "      <td>f5cf8738e8d211095bb89350ed05ee7f9997eb19</td>\n",
       "      <td>By how much do they outperform standard BERT?</td>\n",
       "      <td>[{'unanswerable': False, 'extractive_spans': [...</td>\n",
       "      <td>df4a3d95-a9f7-58ee-8584-48b8ff161556</td>\n",
       "      <td>/mnt/data/wangshu/mmrag/qasper/documents/1909....</td>\n",
       "      <td>In this paper, we focus on the classification ...</td>\n",
       "      <td>{'answer': [{'unanswerable': False, 'extractiv...</td>\n",
       "      <td>{'caption': ['Table 1: Availability of additio...</td>\n",
       "      <td>five</td>\n",
       "      <td>no</td>\n",
       "      <td>5053f146237e8fc8859ed3984b5d3f02f39266b7</td>\n",
       "      <td></td>\n",
       "      <td>Enriching BERT with Knowledge Graph Embeddings...</td>\n",
       "      <td>familiar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1909.08402</td>\n",
       "      <td>bed527bcb0dd5424e69563fba4ae7e6ea1fca26a</td>\n",
       "      <td>What dataset do they use?</td>\n",
       "      <td>[{'unanswerable': False, 'extractive_spans': [...</td>\n",
       "      <td>df4a3d95-a9f7-58ee-8584-48b8ff161556</td>\n",
       "      <td>/mnt/data/wangshu/mmrag/qasper/documents/1909....</td>\n",
       "      <td>In this paper, we focus on the classification ...</td>\n",
       "      <td>{'answer': [{'unanswerable': False, 'extractiv...</td>\n",
       "      <td>{'caption': ['Table 1: Availability of additio...</td>\n",
       "      <td>five</td>\n",
       "      <td>no</td>\n",
       "      <td>5053f146237e8fc8859ed3984b5d3f02f39266b7</td>\n",
       "      <td></td>\n",
       "      <td>Enriching BERT with Knowledge Graph Embeddings...</td>\n",
       "      <td>familiar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                               question_id  \\\n",
       "0  1909.08402  f5cf8738e8d211095bb89350ed05ee7f9997eb19   \n",
       "0  1909.08402  bed527bcb0dd5424e69563fba4ae7e6ea1fca26a   \n",
       "\n",
       "                                        question  \\\n",
       "0  By how much do they outperform standard BERT?   \n",
       "0                      What dataset do they use?   \n",
       "\n",
       "                                              answer  \\\n",
       "0  [{'unanswerable': False, 'extractive_spans': [...   \n",
       "0  [{'unanswerable': False, 'extractive_spans': [...   \n",
       "\n",
       "                               doc_uuid  \\\n",
       "0  df4a3d95-a9f7-58ee-8584-48b8ff161556   \n",
       "0  df4a3d95-a9f7-58ee-8584-48b8ff161556   \n",
       "\n",
       "                                            doc_path  \\\n",
       "0  /mnt/data/wangshu/mmrag/qasper/documents/1909....   \n",
       "0  /mnt/data/wangshu/mmrag/qasper/documents/1909....   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  In this paper, we focus on the classification ...   \n",
       "0  In this paper, we focus on the classification ...   \n",
       "\n",
       "                                         answers_ori  \\\n",
       "0  {'answer': [{'unanswerable': False, 'extractiv...   \n",
       "0  {'answer': [{'unanswerable': False, 'extractiv...   \n",
       "\n",
       "                                  figures_and_tables nlp_background  \\\n",
       "0  {'caption': ['Table 1: Availability of additio...           five   \n",
       "0  {'caption': ['Table 1: Availability of additio...           five   \n",
       "\n",
       "  paper_read                           question_writer search_query  \\\n",
       "0         no  5053f146237e8fc8859ed3984b5d3f02f39266b7                \n",
       "0         no  5053f146237e8fc8859ed3984b5d3f02f39266b7                \n",
       "\n",
       "                                               title topic_background  \n",
       "0  Enriching BERT with Knowledge Graph Embeddings...         familiar  \n",
       "0  Enriching BERT with Knowledge Graph Embeddings...         familiar  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior_cols = [\n",
    "    'id', 'question_id', 'question', 'answer', 'doc_uuid', 'doc_path']\n",
    "\n",
    "reorder = prior_cols + expanded_df.columns.difference(prior_cols).tolist()\n",
    "\n",
    "expanded_df = expanded_df[reorder]\n",
    "\n",
    "print(expanded_df.shape)\n",
    "expanded_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccd51ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"xxxxx/data/qasper.json\"\n",
    "\n",
    "expanded_df.to_json(save_path, orient='records', indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884d0c4d",
   "metadata": {},
   "source": [
    "## dataset info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2ed7cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 1909.00694\n",
      "title: Minimally Supervised Learning of Affective Events Using Discourse Relations\n",
      "abstract: Recognizing affective events that trigger positive or negative sentiment has a wide range of natural language processing applications but remains a challenging problem mainly because the polarity of an event is not necessarily predictable from its constituent words. In this paper, we propose to propagate affective polarity using discourse relations. Our method is simple and only requires a very small seed lexicon and a large raw corpus. Our experiments using Japanese data show that our method learns affective events effectively without manually labeled data. It also improves supervised learning results when labeled data are small.\n",
      "qas: {'question': ['What is the seed lexicon?', 'What are the results?', 'How are relations used to propagate polarity?', 'How big is the Japanese data?', 'What are labels available in dataset for supervision?', 'How big are improvements of supervszed learning results trained on smalled labeled data enhanced with proposed approach copared to basic approach?', 'How does their model learn using mostly raw data?', 'How big is seed lexicon used for training?', 'How large is raw corpus used for training?'], 'question_id': ['753990d0b621d390ed58f20c4d9e4f065f0dc672', '9d578ddccc27dd849244d632dd0f6bf27348ad81', '02e4bf719b1a504e385c35c6186742e720bcb281', '44c4bd6decc86f1091b5fc0728873d9324cdde4e', '86abeff85f3db79cf87a8c993e5e5aa61226dc98', 'c029deb7f99756d2669abad0a349d917428e9c12', '39f8db10d949c6b477fa4b51e7c184016505884f', 'd0bc782961567dc1dd7e074b621a6d6be44bb5b4', 'a592498ba2fac994cd6fad7372836f0adb37e22a'], 'nlp_background': ['two', 'two', 'two', 'two', 'zero', 'zero', 'zero', 'zero', 'zero'], 'topic_background': ['unfamiliar', 'unfamiliar', 'unfamiliar', 'unfamiliar', 'unfamiliar', 'unfamiliar', 'unfamiliar', 'unfamiliar', 'unfamiliar'], 'paper_read': ['no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no'], 'search_query': ['', '', '', '', '', '', '', '', ''], 'question_writer': ['c1fbdd7a261021041f75fbe00a55b4c386ebbbb4', 'c1fbdd7a261021041f75fbe00a55b4c386ebbbb4', 'c1fbdd7a261021041f75fbe00a55b4c386ebbbb4', 'c1fbdd7a261021041f75fbe00a55b4c386ebbbb4', '258ee4069f740c400c0049a2580945a1cc7f044c', '258ee4069f740c400c0049a2580945a1cc7f044c', '258ee4069f740c400c0049a2580945a1cc7f044c', '258ee4069f740c400c0049a2580945a1cc7f044c', '258ee4069f740c400c0049a2580945a1cc7f044c'], 'answers': [{'answer': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'a vocabulary of positive and negative predicates that helps determine the polarity score of an event', 'evidence': ['The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event. We expect the model to automatically learn complex phenomena through label propagation. Based on the availability of scores and the types of discourse relations, we classify the extracted event pairs into the following three types.'], 'highlighted_evidence': ['The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event.', 'It is a ']}, {'unanswerable': False, 'extractive_spans': ['seed lexicon consists of positive and negative predicates'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event. We expect the model to automatically learn complex phenomena through label propagation. Based on the availability of scores and the types of discourse relations, we classify the extracted event pairs into the following three types.'], 'highlighted_evidence': ['The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event.']}], 'annotation_id': ['31e85022a847f37c15fd0415f3c450c74c8e4755', '95da0a6e1b08db74a405c6a71067c9b272a50ff5'], 'worker_id': ['c1fbdd7a261021041f75fbe00a55b4c386ebbbb4', '2cfd959e433f290bb50b55722370f0d22fe090b7']}, {'answer': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'Using all data to train: AL -- BiGRU achieved 0.843 accuracy, AL -- BERT achieved 0.863 accuracy, AL+CA+CO -- BiGRU achieved 0.866 accuracy, AL+CA+CO -- BERT achieved 0.835, accuracy, ACP -- BiGRU achieved 0.919 accuracy, ACP -- BERT achived 0.933, accuracy, ACP+AL+CA+CO -- BiGRU achieved 0.917 accuracy, ACP+AL+CA+CO -- BERT achieved 0.913 accuracy. \\nUsing a subset to train: BERT achieved 0.876 accuracy using ACP (6K), BERT achieved 0.886 accuracy using ACP (6K) + AL, BiGRU achieved 0.830 accuracy using ACP (6K), BiGRU achieved 0.879 accuracy using ACP (6K) + AL + CA + CO.', 'evidence': ['FLOAT SELECTED: Table 3: Performance of various models on the ACP test set.', 'FLOAT SELECTED: Table 4: Results for small labeled training data. Given the performance with the full dataset, we show BERT trained only with the AL data.', 'As for ${\\\\rm Encoder}$, we compared two types of neural networks: BiGRU and BERT. GRU BIBREF16 is a recurrent neural network sequence encoder. BiGRU reads an input sequence forward and backward and the output is the concatenation of the final forward and backward hidden states.', 'We trained the model with the following four combinations of the datasets: AL, AL+CA+CO (two proposed models), ACP (supervised), and ACP+AL+CA+CO (semi-supervised). The corresponding objective functions were: $\\\\mathcal {L}_{\\\\rm AL}$, $\\\\mathcal {L}_{\\\\rm AL} + \\\\mathcal {L}_{\\\\rm CA} + \\\\mathcal {L}_{\\\\rm CO}$, $\\\\mathcal {L}_{\\\\rm ACP}$, and $\\\\mathcal {L}_{\\\\rm ACP} + \\\\mathcal {L}_{\\\\rm AL} + \\\\mathcal {L}_{\\\\rm CA} + \\\\mathcal {L}_{\\\\rm CO}$.'], 'highlighted_evidence': ['FLOAT SELECTED: Table 3: Performance of various models on the ACP test set.', 'FLOAT SELECTED: Table 4: Results for small labeled training data. Given the performance with the full dataset, we show BERT trained only with the AL data.', 'As for ${\\\\rm Encoder}$, we compared two types of neural networks: BiGRU and BERT. ', 'We trained the model with the following four combinations of the datasets: AL, AL+CA+CO (two proposed models), ACP (supervised), and ACP+AL+CA+CO (semi-supervised). The corresponding objective functions were: $\\\\mathcal {L}_{\\\\rm AL}$, $\\\\mathcal {L}_{\\\\rm AL} + \\\\mathcal {L}_{\\\\rm CA} + \\\\mathcal {L}_{\\\\rm CO}$, $\\\\mathcal {L}_{\\\\rm ACP}$, and $\\\\mathcal {L}_{\\\\rm ACP} + \\\\mathcal {L}_{\\\\rm AL} + \\\\mathcal {L}_{\\\\rm CA} + \\\\mathcal {L}_{\\\\rm CO}$.']}], 'annotation_id': ['1e5e867244ea656c4b7632628086209cf9bae5fa'], 'worker_id': ['2cfd959e433f290bb50b55722370f0d22fe090b7']}, {'answer': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'based on the relation between events, the suggested polarity of one event can determine the possible polarity of the other event ', 'evidence': [\"In this paper, we propose a simple and effective method for learning affective events that only requires a very small seed lexicon and a large raw corpus. As illustrated in Figure FIGREF1, our key idea is that we can exploit discourse relations BIBREF4 to efficiently propagate polarity from seed predicates that directly report one's emotions (e.g., “to be glad” is positive). Suppose that events $x_1$ are $x_2$ are in the discourse relation of Cause (i.e., $x_1$ causes $x_2$). If the seed lexicon suggests $x_2$ is positive, $x_1$ is also likely to be positive because it triggers the positive emotion. The fact that $x_2$ is known to be negative indicates the negative polarity of $x_1$. Similarly, if $x_1$ and $x_2$ are in the discourse relation of Concession (i.e., $x_2$ in spite of $x_1$), the reverse of $x_2$'s polarity can be propagated to $x_1$. Even if $x_2$'s polarity is not known in advance, we can exploit the tendency of $x_1$ and $x_2$ to be of the same polarity (for Cause) or of the reverse polarity (for Concession) although the heuristic is not exempt from counterexamples. We transform this idea into objective functions and train neural network models that predict the polarity of a given event.\"], 'highlighted_evidence': [\"As illustrated in Figure FIGREF1, our key idea is that we can exploit discourse relations BIBREF4 to efficiently propagate polarity from seed predicates that directly report one's emotions (e.g., “to be glad” is positive). Suppose that events $x_1$ are $x_2$ are in the discourse relation of Cause (i.e., $x_1$ causes $x_2$). If the seed lexicon suggests $x_2$ is positive, $x_1$ is also likely to be positive because it triggers the positive emotion. The fact that $x_2$ is known to be negative indicates the negative polarity of $x_1$. Similarly, if $x_1$ and $x_2$ are in the discourse relation of Concession (i.e., $x_2$ in spite of $x_1$), the reverse of $x_2$'s polarity can be propagated to $x_1$. Even if $x_2$'s polarity is not known in advance, we can exploit the tendency of $x_1$ and $x_2$ to be of the same polarity (for Cause) or of the reverse polarity (for Concession) although the heuristic is not exempt from counterexamples. We transform this idea into objective functions and train neural network models that predict the polarity of a given event.\"]}, {'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'cause relation: both events in the relation should have the same polarity; concession relation: events should have opposite polarity', 'evidence': [\"In this paper, we propose a simple and effective method for learning affective events that only requires a very small seed lexicon and a large raw corpus. As illustrated in Figure FIGREF1, our key idea is that we can exploit discourse relations BIBREF4 to efficiently propagate polarity from seed predicates that directly report one's emotions (e.g., “to be glad” is positive). Suppose that events $x_1$ are $x_2$ are in the discourse relation of Cause (i.e., $x_1$ causes $x_2$). If the seed lexicon suggests $x_2$ is positive, $x_1$ is also likely to be positive because it triggers the positive emotion. The fact that $x_2$ is known to be negative indicates the negative polarity of $x_1$. Similarly, if $x_1$ and $x_2$ are in the discourse relation of Concession (i.e., $x_2$ in spite of $x_1$), the reverse of $x_2$'s polarity can be propagated to $x_1$. Even if $x_2$'s polarity is not known in advance, we can exploit the tendency of $x_1$ and $x_2$ to be of the same polarity (for Cause) or of the reverse polarity (for Concession) although the heuristic is not exempt from counterexamples. We transform this idea into objective functions and train neural network models that predict the polarity of a given event.\", 'The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event. We expect the model to automatically learn complex phenomena through label propagation. Based on the availability of scores and the types of discourse relations, we classify the extracted event pairs into the following three types.'], 'highlighted_evidence': [\"As illustrated in Figure FIGREF1, our key idea is that we can exploit discourse relations BIBREF4 to efficiently propagate polarity from seed predicates that directly report one's emotions (e.g., “to be glad” is positive). Suppose that events $x_1$ are $x_2$ are in the discourse relation of Cause (i.e., $x_1$ causes $x_2$). If the seed lexicon suggests $x_2$ is positive, $x_1$ is also likely to be positive because it triggers the positive emotion. The fact that $x_2$ is known to be negative indicates the negative polarity of $x_1$. Similarly, if $x_1$ and $x_2$ are in the discourse relation of Concession (i.e., $x_2$ in spite of $x_1$), the reverse of $x_2$'s polarity can be propagated to $x_1$. Even if $x_2$'s polarity is not known in advance, we can exploit the tendency of $x_1$ and $x_2$ to be of the same polarity (for Cause) or of the reverse polarity (for Concession) although the heuristic is not exempt from counterexamples. We transform this idea into objective functions and train neural network models that predict the polarity of a given event.\", 'The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event. We expect the model to automatically learn complex phenomena through label propagation.']}], 'annotation_id': ['49a78a07d2eed545556a835ccf2eb40e5eee9801', 'acd6d15bd67f4b1496ee8af1c93c33e7d59c89e1'], 'worker_id': ['c1fbdd7a261021041f75fbe00a55b4c386ebbbb4', '2cfd959e433f290bb50b55722370f0d22fe090b7']}, {'answer': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '7000000 pairs of events were extracted from the Japanese Web corpus, 529850 pairs of events were extracted from the ACP corpus', 'evidence': ['As a raw corpus, we used a Japanese web corpus that was compiled through the procedures proposed by BIBREF13. To extract event pairs tagged with discourse relations, we used the Japanese dependency parser KNP and in-house postprocessing scripts BIBREF14. KNP used hand-written rules to segment each sentence into what we conventionally called clauses (mostly consecutive text chunks), each of which contained one main predicate. KNP also identified the discourse relations of event pairs if explicit discourse connectives BIBREF4 such as “ので” (because) and “のに” (in spite of) were present. We treated Cause/Reason (原因・理由) and Condition (条件) in the original tagset BIBREF15 as Cause and Concession (逆接) as Concession, respectively. Here is an example of event pair extraction.', 'We constructed our seed lexicon consisting of 15 positive words and 15 negative words, as shown in Section SECREF27. From the corpus of about 100 million sentences, we obtained 1.4 millions event pairs for AL, 41 millions for CA, and 6 millions for CO. We randomly selected subsets of AL event pairs such that positive and negative latter events were equal in size. We also sampled event pairs for each of CA and CO such that it was five times larger than AL. The results are shown in Table TABREF16.', 'FLOAT SELECTED: Table 1: Statistics of the AL, CA, and CO datasets.', 'We used the latest version of the ACP Corpus BIBREF12 for evaluation. It was used for (semi-)supervised training as well. Extracted from Japanese websites using HTML layouts and linguistic patterns, the dataset covered various genres. For example, the following two sentences were labeled positive and negative, respectively:', 'Although the ACP corpus was originally constructed in the context of sentiment analysis, we found that it could roughly be regarded as a collection of affective events. We parsed each sentence and extracted the last clause in it. The train/dev/test split of the data is shown in Table TABREF19.', 'FLOAT SELECTED: Table 2: Details of the ACP dataset.'], 'highlighted_evidence': ['As a raw corpus, we used a Japanese web corpus that was compiled through the procedures proposed by BIBREF13. ', 'From the corpus of about 100 million sentences, we obtained 1.4 millions event pairs for AL, 41 millions for CA, and 6 millions for CO. We randomly selected subsets of AL event pairs such that positive and negative latter events were equal in size. We also sampled event pairs for each of CA and CO such that it was five times larger than AL. The results are shown in Table TABREF16.', 'FLOAT SELECTED: Table 1: Statistics of the AL, CA, and CO datasets.', 'We used the latest version of the ACP Corpus BIBREF12 for evaluation. It was used for (semi-)supervised training as well.', 'Although the ACP corpus was originally constructed in the context of sentiment analysis, we found that it could roughly be regarded as a collection of affective events. We parsed each sentence and extracted the last clause in it. The train/dev/test split of the data is shown in Table TABREF19.', 'FLOAT SELECTED: Table 2: Details of the ACP dataset.']}, {'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'The ACP corpus has around 700k events split into positive and negative polarity ', 'evidence': ['FLOAT SELECTED: Table 2: Details of the ACP dataset.'], 'highlighted_evidence': ['FLOAT SELECTED: Table 2: Details of the ACP dataset.']}], 'annotation_id': ['36926a4c9e14352c91111150aa4c6edcc5c0770f', '75b6dd28ccab20a70087635d89c2b22d0e99095c'], 'worker_id': ['2cfd959e433f290bb50b55722370f0d22fe090b7', 'c1fbdd7a261021041f75fbe00a55b4c386ebbbb4']}, {'answer': [{'unanswerable': False, 'extractive_spans': ['negative', 'positive'], 'yes_no': None, 'free_form_answer': '', 'evidence': [\"Affective events BIBREF0 are events that typically affect people in positive or negative ways. For example, getting money and playing sports are usually positive to the experiencers; catching cold and losing one's wallet are negative. Understanding affective events is important to various natural language processing (NLP) applications such as dialogue systems BIBREF1, question-answering systems BIBREF2, and humor recognition BIBREF3. In this paper, we work on recognizing the polarity of an affective event that is represented by a score ranging from $-1$ (negative) to 1 (positive).\"], 'highlighted_evidence': ['In this paper, we work on recognizing the polarity of an affective event that is represented by a score ranging from $-1$ (negative) to 1 (positive).']}], 'annotation_id': ['2d8c7df145c37aad905e48f64d8caa69e54434d4'], 'worker_id': ['c1018a31c3272ce74964a3280069f62f314a1a58']}, {'answer': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '3%', 'evidence': ['FLOAT SELECTED: Table 4: Results for small labeled training data. Given the performance with the full dataset, we show BERT trained only with the AL data.'], 'highlighted_evidence': ['FLOAT SELECTED: Table 4: Results for small labeled training data. Given the performance with the full dataset, we show BERT trained only with the AL data.']}], 'annotation_id': ['df4372b2e8d9bb2039a5582f192768953b01d904'], 'worker_id': ['c1018a31c3272ce74964a3280069f62f314a1a58']}, {'answer': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'by exploiting discourse relations to propagate polarity from seed predicates to final sentiment polarity', 'evidence': [\"In this paper, we propose a simple and effective method for learning affective events that only requires a very small seed lexicon and a large raw corpus. As illustrated in Figure FIGREF1, our key idea is that we can exploit discourse relations BIBREF4 to efficiently propagate polarity from seed predicates that directly report one's emotions (e.g., “to be glad” is positive). Suppose that events $x_1$ are $x_2$ are in the discourse relation of Cause (i.e., $x_1$ causes $x_2$). If the seed lexicon suggests $x_2$ is positive, $x_1$ is also likely to be positive because it triggers the positive emotion. The fact that $x_2$ is known to be negative indicates the negative polarity of $x_1$. Similarly, if $x_1$ and $x_2$ are in the discourse relation of Concession (i.e., $x_2$ in spite of $x_1$), the reverse of $x_2$'s polarity can be propagated to $x_1$. Even if $x_2$'s polarity is not known in advance, we can exploit the tendency of $x_1$ and $x_2$ to be of the same polarity (for Cause) or of the reverse polarity (for Concession) although the heuristic is not exempt from counterexamples. We transform this idea into objective functions and train neural network models that predict the polarity of a given event.\"], 'highlighted_evidence': [\"In this paper, we propose a simple and effective method for learning affective events that only requires a very small seed lexicon and a large raw corpus. As illustrated in Figure FIGREF1, our key idea is that we can exploit discourse relations BIBREF4 to efficiently propagate polarity from seed predicates that directly report one's emotions (e.g., “to be glad” is positive).\"]}], 'annotation_id': ['5c5bbc8af91c16af89b4ddd57ee6834be018e4e7'], 'worker_id': ['c1018a31c3272ce74964a3280069f62f314a1a58']}, {'answer': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '30 words', 'evidence': ['We constructed our seed lexicon consisting of 15 positive words and 15 negative words, as shown in Section SECREF27. From the corpus of about 100 million sentences, we obtained 1.4 millions event pairs for AL, 41 millions for CA, and 6 millions for CO. We randomly selected subsets of AL event pairs such that positive and negative latter events were equal in size. We also sampled event pairs for each of CA and CO such that it was five times larger than AL. The results are shown in Table TABREF16.'], 'highlighted_evidence': ['We constructed our seed lexicon consisting of 15 positive words and 15 negative words, as shown in Section SECREF27. ']}], 'annotation_id': ['0206f2131f64a3e02498cedad1250971b78ffd0c'], 'worker_id': ['c1018a31c3272ce74964a3280069f62f314a1a58']}, {'answer': [{'unanswerable': False, 'extractive_spans': ['100 million sentences'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['As a raw corpus, we used a Japanese web corpus that was compiled through the procedures proposed by BIBREF13. To extract event pairs tagged with discourse relations, we used the Japanese dependency parser KNP and in-house postprocessing scripts BIBREF14. KNP used hand-written rules to segment each sentence into what we conventionally called clauses (mostly consecutive text chunks), each of which contained one main predicate. KNP also identified the discourse relations of event pairs if explicit discourse connectives BIBREF4 such as “ので” (because) and “のに” (in spite of) were present. We treated Cause/Reason (原因・理由) and Condition (条件) in the original tagset BIBREF15 as Cause and Concession (逆接) as Concession, respectively. Here is an example of event pair extraction.', 'We constructed our seed lexicon consisting of 15 positive words and 15 negative words, as shown in Section SECREF27. From the corpus of about 100 million sentences, we obtained 1.4 millions event pairs for AL, 41 millions for CA, and 6 millions for CO. We randomly selected subsets of AL event pairs such that positive and negative latter events were equal in size. We also sampled event pairs for each of CA and CO such that it was five times larger than AL. The results are shown in Table TABREF16.'], 'highlighted_evidence': ['As a raw corpus, we used a Japanese web corpus that was compiled through the procedures proposed by BIBREF13. ', 'From the corpus of about 100 million sentences, we obtained 1.4 millions event pairs for AL, 41 millions for CA, and 6 millions for CO.']}], 'annotation_id': ['c36bad2758c4f9866d64c357c475d370595d937f'], 'worker_id': ['c1018a31c3272ce74964a3280069f62f314a1a58']}]}\n",
      "figures_and_tables: {'caption': ['Figure 1: An overview of our method. We focus on pairs of events, the former events and the latter events, which are connected with a discourse relation, CAUSE or CONCESSION. Dropped pronouns are indicated by brackets in English translations. We divide the event pairs into three types: AL, CA, and CO. In AL, the polarity of a latter event is automatically identified as either positive or negative, according to the seed lexicon (the positive word is colored red and the negative word blue). We propagate the latter event’s polarity to the former event. The same polarity as the latter event is used for the discourse relation CAUSE, and the reversed polarity for CONCESSION. In CA and CO, the latter event’s polarity is not known. Depending on the discourse relation, we encourage the two events’ polarities to be the same (CA) or reversed (CO). Details are given in Section 3.2.', 'Table 1: Statistics of the AL, CA, and CO datasets.', 'Table 2: Details of the ACP dataset.', 'Table 5: Examples of polarity scores predicted by the BiGRU model trained with AL+CA+CO.', 'Table 3: Performance of various models on the ACP test set.', 'Table 4: Results for small labeled training data. Given the performance with the full dataset, we show BERT trained only with the AL data.'], 'file': ['2-Figure1-1.png', '4-Table1-1.png', '4-Table2-1.png', '5-Table5-1.png', '5-Table3-1.png', '5-Table4-1.png']}\n"
     ]
    }
   ],
   "source": [
    "for example in qasper_dataset['train']:\n",
    "    for k, v in example.items():\n",
    "        if k == \"full_text\":\n",
    "            continue\n",
    "        print(f\"{k}: {v}\")\n",
    "        if k == \"qas\":\n",
    "            qas = v\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ec8efab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question, len 9\n",
      "question_id, len 9\n",
      "nlp_background, len 9\n",
      "topic_background, len 9\n",
      "paper_read, len 9\n",
      "search_query, len 9\n",
      "question_writer, len 9\n",
      "answers, len 9\n",
      "dict_keys(['answer', 'annotation_id', 'worker_id'])\n",
      "Q: What is the seed lexicon?\n",
      "A: {'answer': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'a vocabulary of positive and negative predicates that helps determine the polarity score of an event', 'evidence': ['The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event. We expect the model to automatically learn complex phenomena through label propagation. Based on the availability of scores and the types of discourse relations, we classify the extracted event pairs into the following three types.'], 'highlighted_evidence': ['The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event.', 'It is a ']}, {'unanswerable': False, 'extractive_spans': ['seed lexicon consists of positive and negative predicates'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event. We expect the model to automatically learn complex phenomena through label propagation. Based on the availability of scores and the types of discourse relations, we classify the extracted event pairs into the following three types.'], 'highlighted_evidence': ['The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event.']}], 'annotation_id': ['31e85022a847f37c15fd0415f3c450c74c8e4755', '95da0a6e1b08db74a405c6a71067c9b272a50ff5'], 'worker_id': ['c1fbdd7a261021041f75fbe00a55b4c386ebbbb4', '2cfd959e433f290bb50b55722370f0d22fe090b7']}\n",
      "Q: What are the results?\n",
      "A: {'answer': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'Using all data to train: AL -- BiGRU achieved 0.843 accuracy, AL -- BERT achieved 0.863 accuracy, AL+CA+CO -- BiGRU achieved 0.866 accuracy, AL+CA+CO -- BERT achieved 0.835, accuracy, ACP -- BiGRU achieved 0.919 accuracy, ACP -- BERT achived 0.933, accuracy, ACP+AL+CA+CO -- BiGRU achieved 0.917 accuracy, ACP+AL+CA+CO -- BERT achieved 0.913 accuracy. \\nUsing a subset to train: BERT achieved 0.876 accuracy using ACP (6K), BERT achieved 0.886 accuracy using ACP (6K) + AL, BiGRU achieved 0.830 accuracy using ACP (6K), BiGRU achieved 0.879 accuracy using ACP (6K) + AL + CA + CO.', 'evidence': ['FLOAT SELECTED: Table 3: Performance of various models on the ACP test set.', 'FLOAT SELECTED: Table 4: Results for small labeled training data. Given the performance with the full dataset, we show BERT trained only with the AL data.', 'As for ${\\\\rm Encoder}$, we compared two types of neural networks: BiGRU and BERT. GRU BIBREF16 is a recurrent neural network sequence encoder. BiGRU reads an input sequence forward and backward and the output is the concatenation of the final forward and backward hidden states.', 'We trained the model with the following four combinations of the datasets: AL, AL+CA+CO (two proposed models), ACP (supervised), and ACP+AL+CA+CO (semi-supervised). The corresponding objective functions were: $\\\\mathcal {L}_{\\\\rm AL}$, $\\\\mathcal {L}_{\\\\rm AL} + \\\\mathcal {L}_{\\\\rm CA} + \\\\mathcal {L}_{\\\\rm CO}$, $\\\\mathcal {L}_{\\\\rm ACP}$, and $\\\\mathcal {L}_{\\\\rm ACP} + \\\\mathcal {L}_{\\\\rm AL} + \\\\mathcal {L}_{\\\\rm CA} + \\\\mathcal {L}_{\\\\rm CO}$.'], 'highlighted_evidence': ['FLOAT SELECTED: Table 3: Performance of various models on the ACP test set.', 'FLOAT SELECTED: Table 4: Results for small labeled training data. Given the performance with the full dataset, we show BERT trained only with the AL data.', 'As for ${\\\\rm Encoder}$, we compared two types of neural networks: BiGRU and BERT. ', 'We trained the model with the following four combinations of the datasets: AL, AL+CA+CO (two proposed models), ACP (supervised), and ACP+AL+CA+CO (semi-supervised). The corresponding objective functions were: $\\\\mathcal {L}_{\\\\rm AL}$, $\\\\mathcal {L}_{\\\\rm AL} + \\\\mathcal {L}_{\\\\rm CA} + \\\\mathcal {L}_{\\\\rm CO}$, $\\\\mathcal {L}_{\\\\rm ACP}$, and $\\\\mathcal {L}_{\\\\rm ACP} + \\\\mathcal {L}_{\\\\rm AL} + \\\\mathcal {L}_{\\\\rm CA} + \\\\mathcal {L}_{\\\\rm CO}$.']}], 'annotation_id': ['1e5e867244ea656c4b7632628086209cf9bae5fa'], 'worker_id': ['2cfd959e433f290bb50b55722370f0d22fe090b7']}\n",
      "Q: How are relations used to propagate polarity?\n",
      "A: {'answer': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'based on the relation between events, the suggested polarity of one event can determine the possible polarity of the other event ', 'evidence': [\"In this paper, we propose a simple and effective method for learning affective events that only requires a very small seed lexicon and a large raw corpus. As illustrated in Figure FIGREF1, our key idea is that we can exploit discourse relations BIBREF4 to efficiently propagate polarity from seed predicates that directly report one's emotions (e.g., “to be glad” is positive). Suppose that events $x_1$ are $x_2$ are in the discourse relation of Cause (i.e., $x_1$ causes $x_2$). If the seed lexicon suggests $x_2$ is positive, $x_1$ is also likely to be positive because it triggers the positive emotion. The fact that $x_2$ is known to be negative indicates the negative polarity of $x_1$. Similarly, if $x_1$ and $x_2$ are in the discourse relation of Concession (i.e., $x_2$ in spite of $x_1$), the reverse of $x_2$'s polarity can be propagated to $x_1$. Even if $x_2$'s polarity is not known in advance, we can exploit the tendency of $x_1$ and $x_2$ to be of the same polarity (for Cause) or of the reverse polarity (for Concession) although the heuristic is not exempt from counterexamples. We transform this idea into objective functions and train neural network models that predict the polarity of a given event.\"], 'highlighted_evidence': [\"As illustrated in Figure FIGREF1, our key idea is that we can exploit discourse relations BIBREF4 to efficiently propagate polarity from seed predicates that directly report one's emotions (e.g., “to be glad” is positive). Suppose that events $x_1$ are $x_2$ are in the discourse relation of Cause (i.e., $x_1$ causes $x_2$). If the seed lexicon suggests $x_2$ is positive, $x_1$ is also likely to be positive because it triggers the positive emotion. The fact that $x_2$ is known to be negative indicates the negative polarity of $x_1$. Similarly, if $x_1$ and $x_2$ are in the discourse relation of Concession (i.e., $x_2$ in spite of $x_1$), the reverse of $x_2$'s polarity can be propagated to $x_1$. Even if $x_2$'s polarity is not known in advance, we can exploit the tendency of $x_1$ and $x_2$ to be of the same polarity (for Cause) or of the reverse polarity (for Concession) although the heuristic is not exempt from counterexamples. We transform this idea into objective functions and train neural network models that predict the polarity of a given event.\"]}, {'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'cause relation: both events in the relation should have the same polarity; concession relation: events should have opposite polarity', 'evidence': [\"In this paper, we propose a simple and effective method for learning affective events that only requires a very small seed lexicon and a large raw corpus. As illustrated in Figure FIGREF1, our key idea is that we can exploit discourse relations BIBREF4 to efficiently propagate polarity from seed predicates that directly report one's emotions (e.g., “to be glad” is positive). Suppose that events $x_1$ are $x_2$ are in the discourse relation of Cause (i.e., $x_1$ causes $x_2$). If the seed lexicon suggests $x_2$ is positive, $x_1$ is also likely to be positive because it triggers the positive emotion. The fact that $x_2$ is known to be negative indicates the negative polarity of $x_1$. Similarly, if $x_1$ and $x_2$ are in the discourse relation of Concession (i.e., $x_2$ in spite of $x_1$), the reverse of $x_2$'s polarity can be propagated to $x_1$. Even if $x_2$'s polarity is not known in advance, we can exploit the tendency of $x_1$ and $x_2$ to be of the same polarity (for Cause) or of the reverse polarity (for Concession) although the heuristic is not exempt from counterexamples. We transform this idea into objective functions and train neural network models that predict the polarity of a given event.\", 'The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event. We expect the model to automatically learn complex phenomena through label propagation. Based on the availability of scores and the types of discourse relations, we classify the extracted event pairs into the following three types.'], 'highlighted_evidence': [\"As illustrated in Figure FIGREF1, our key idea is that we can exploit discourse relations BIBREF4 to efficiently propagate polarity from seed predicates that directly report one's emotions (e.g., “to be glad” is positive). Suppose that events $x_1$ are $x_2$ are in the discourse relation of Cause (i.e., $x_1$ causes $x_2$). If the seed lexicon suggests $x_2$ is positive, $x_1$ is also likely to be positive because it triggers the positive emotion. The fact that $x_2$ is known to be negative indicates the negative polarity of $x_1$. Similarly, if $x_1$ and $x_2$ are in the discourse relation of Concession (i.e., $x_2$ in spite of $x_1$), the reverse of $x_2$'s polarity can be propagated to $x_1$. Even if $x_2$'s polarity is not known in advance, we can exploit the tendency of $x_1$ and $x_2$ to be of the same polarity (for Cause) or of the reverse polarity (for Concession) although the heuristic is not exempt from counterexamples. We transform this idea into objective functions and train neural network models that predict the polarity of a given event.\", 'The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event. We expect the model to automatically learn complex phenomena through label propagation.']}], 'annotation_id': ['49a78a07d2eed545556a835ccf2eb40e5eee9801', 'acd6d15bd67f4b1496ee8af1c93c33e7d59c89e1'], 'worker_id': ['c1fbdd7a261021041f75fbe00a55b4c386ebbbb4', '2cfd959e433f290bb50b55722370f0d22fe090b7']}\n",
      "Q: How big is the Japanese data?\n",
      "A: {'answer': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '7000000 pairs of events were extracted from the Japanese Web corpus, 529850 pairs of events were extracted from the ACP corpus', 'evidence': ['As a raw corpus, we used a Japanese web corpus that was compiled through the procedures proposed by BIBREF13. To extract event pairs tagged with discourse relations, we used the Japanese dependency parser KNP and in-house postprocessing scripts BIBREF14. KNP used hand-written rules to segment each sentence into what we conventionally called clauses (mostly consecutive text chunks), each of which contained one main predicate. KNP also identified the discourse relations of event pairs if explicit discourse connectives BIBREF4 such as “ので” (because) and “のに” (in spite of) were present. We treated Cause/Reason (原因・理由) and Condition (条件) in the original tagset BIBREF15 as Cause and Concession (逆接) as Concession, respectively. Here is an example of event pair extraction.', 'We constructed our seed lexicon consisting of 15 positive words and 15 negative words, as shown in Section SECREF27. From the corpus of about 100 million sentences, we obtained 1.4 millions event pairs for AL, 41 millions for CA, and 6 millions for CO. We randomly selected subsets of AL event pairs such that positive and negative latter events were equal in size. We also sampled event pairs for each of CA and CO such that it was five times larger than AL. The results are shown in Table TABREF16.', 'FLOAT SELECTED: Table 1: Statistics of the AL, CA, and CO datasets.', 'We used the latest version of the ACP Corpus BIBREF12 for evaluation. It was used for (semi-)supervised training as well. Extracted from Japanese websites using HTML layouts and linguistic patterns, the dataset covered various genres. For example, the following two sentences were labeled positive and negative, respectively:', 'Although the ACP corpus was originally constructed in the context of sentiment analysis, we found that it could roughly be regarded as a collection of affective events. We parsed each sentence and extracted the last clause in it. The train/dev/test split of the data is shown in Table TABREF19.', 'FLOAT SELECTED: Table 2: Details of the ACP dataset.'], 'highlighted_evidence': ['As a raw corpus, we used a Japanese web corpus that was compiled through the procedures proposed by BIBREF13. ', 'From the corpus of about 100 million sentences, we obtained 1.4 millions event pairs for AL, 41 millions for CA, and 6 millions for CO. We randomly selected subsets of AL event pairs such that positive and negative latter events were equal in size. We also sampled event pairs for each of CA and CO such that it was five times larger than AL. The results are shown in Table TABREF16.', 'FLOAT SELECTED: Table 1: Statistics of the AL, CA, and CO datasets.', 'We used the latest version of the ACP Corpus BIBREF12 for evaluation. It was used for (semi-)supervised training as well.', 'Although the ACP corpus was originally constructed in the context of sentiment analysis, we found that it could roughly be regarded as a collection of affective events. We parsed each sentence and extracted the last clause in it. The train/dev/test split of the data is shown in Table TABREF19.', 'FLOAT SELECTED: Table 2: Details of the ACP dataset.']}, {'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'The ACP corpus has around 700k events split into positive and negative polarity ', 'evidence': ['FLOAT SELECTED: Table 2: Details of the ACP dataset.'], 'highlighted_evidence': ['FLOAT SELECTED: Table 2: Details of the ACP dataset.']}], 'annotation_id': ['36926a4c9e14352c91111150aa4c6edcc5c0770f', '75b6dd28ccab20a70087635d89c2b22d0e99095c'], 'worker_id': ['2cfd959e433f290bb50b55722370f0d22fe090b7', 'c1fbdd7a261021041f75fbe00a55b4c386ebbbb4']}\n",
      "Q: What are labels available in dataset for supervision?\n",
      "A: {'answer': [{'unanswerable': False, 'extractive_spans': ['negative', 'positive'], 'yes_no': None, 'free_form_answer': '', 'evidence': [\"Affective events BIBREF0 are events that typically affect people in positive or negative ways. For example, getting money and playing sports are usually positive to the experiencers; catching cold and losing one's wallet are negative. Understanding affective events is important to various natural language processing (NLP) applications such as dialogue systems BIBREF1, question-answering systems BIBREF2, and humor recognition BIBREF3. In this paper, we work on recognizing the polarity of an affective event that is represented by a score ranging from $-1$ (negative) to 1 (positive).\"], 'highlighted_evidence': ['In this paper, we work on recognizing the polarity of an affective event that is represented by a score ranging from $-1$ (negative) to 1 (positive).']}], 'annotation_id': ['2d8c7df145c37aad905e48f64d8caa69e54434d4'], 'worker_id': ['c1018a31c3272ce74964a3280069f62f314a1a58']}\n",
      "Q: How big are improvements of supervszed learning results trained on smalled labeled data enhanced with proposed approach copared to basic approach?\n",
      "A: {'answer': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '3%', 'evidence': ['FLOAT SELECTED: Table 4: Results for small labeled training data. Given the performance with the full dataset, we show BERT trained only with the AL data.'], 'highlighted_evidence': ['FLOAT SELECTED: Table 4: Results for small labeled training data. Given the performance with the full dataset, we show BERT trained only with the AL data.']}], 'annotation_id': ['df4372b2e8d9bb2039a5582f192768953b01d904'], 'worker_id': ['c1018a31c3272ce74964a3280069f62f314a1a58']}\n",
      "Q: How does their model learn using mostly raw data?\n",
      "A: {'answer': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': 'by exploiting discourse relations to propagate polarity from seed predicates to final sentiment polarity', 'evidence': [\"In this paper, we propose a simple and effective method for learning affective events that only requires a very small seed lexicon and a large raw corpus. As illustrated in Figure FIGREF1, our key idea is that we can exploit discourse relations BIBREF4 to efficiently propagate polarity from seed predicates that directly report one's emotions (e.g., “to be glad” is positive). Suppose that events $x_1$ are $x_2$ are in the discourse relation of Cause (i.e., $x_1$ causes $x_2$). If the seed lexicon suggests $x_2$ is positive, $x_1$ is also likely to be positive because it triggers the positive emotion. The fact that $x_2$ is known to be negative indicates the negative polarity of $x_1$. Similarly, if $x_1$ and $x_2$ are in the discourse relation of Concession (i.e., $x_2$ in spite of $x_1$), the reverse of $x_2$'s polarity can be propagated to $x_1$. Even if $x_2$'s polarity is not known in advance, we can exploit the tendency of $x_1$ and $x_2$ to be of the same polarity (for Cause) or of the reverse polarity (for Concession) although the heuristic is not exempt from counterexamples. We transform this idea into objective functions and train neural network models that predict the polarity of a given event.\"], 'highlighted_evidence': [\"In this paper, we propose a simple and effective method for learning affective events that only requires a very small seed lexicon and a large raw corpus. As illustrated in Figure FIGREF1, our key idea is that we can exploit discourse relations BIBREF4 to efficiently propagate polarity from seed predicates that directly report one's emotions (e.g., “to be glad” is positive).\"]}], 'annotation_id': ['5c5bbc8af91c16af89b4ddd57ee6834be018e4e7'], 'worker_id': ['c1018a31c3272ce74964a3280069f62f314a1a58']}\n",
      "Q: How big is seed lexicon used for training?\n",
      "A: {'answer': [{'unanswerable': False, 'extractive_spans': [], 'yes_no': None, 'free_form_answer': '30 words', 'evidence': ['We constructed our seed lexicon consisting of 15 positive words and 15 negative words, as shown in Section SECREF27. From the corpus of about 100 million sentences, we obtained 1.4 millions event pairs for AL, 41 millions for CA, and 6 millions for CO. We randomly selected subsets of AL event pairs such that positive and negative latter events were equal in size. We also sampled event pairs for each of CA and CO such that it was five times larger than AL. The results are shown in Table TABREF16.'], 'highlighted_evidence': ['We constructed our seed lexicon consisting of 15 positive words and 15 negative words, as shown in Section SECREF27. ']}], 'annotation_id': ['0206f2131f64a3e02498cedad1250971b78ffd0c'], 'worker_id': ['c1018a31c3272ce74964a3280069f62f314a1a58']}\n",
      "Q: How large is raw corpus used for training?\n",
      "A: {'answer': [{'unanswerable': False, 'extractive_spans': ['100 million sentences'], 'yes_no': None, 'free_form_answer': '', 'evidence': ['As a raw corpus, we used a Japanese web corpus that was compiled through the procedures proposed by BIBREF13. To extract event pairs tagged with discourse relations, we used the Japanese dependency parser KNP and in-house postprocessing scripts BIBREF14. KNP used hand-written rules to segment each sentence into what we conventionally called clauses (mostly consecutive text chunks), each of which contained one main predicate. KNP also identified the discourse relations of event pairs if explicit discourse connectives BIBREF4 such as “ので” (because) and “のに” (in spite of) were present. We treated Cause/Reason (原因・理由) and Condition (条件) in the original tagset BIBREF15 as Cause and Concession (逆接) as Concession, respectively. Here is an example of event pair extraction.', 'We constructed our seed lexicon consisting of 15 positive words and 15 negative words, as shown in Section SECREF27. From the corpus of about 100 million sentences, we obtained 1.4 millions event pairs for AL, 41 millions for CA, and 6 millions for CO. We randomly selected subsets of AL event pairs such that positive and negative latter events were equal in size. We also sampled event pairs for each of CA and CO such that it was five times larger than AL. The results are shown in Table TABREF16.'], 'highlighted_evidence': ['As a raw corpus, we used a Japanese web corpus that was compiled through the procedures proposed by BIBREF13. ', 'From the corpus of about 100 million sentences, we obtained 1.4 millions event pairs for AL, 41 millions for CA, and 6 millions for CO.']}], 'annotation_id': ['c36bad2758c4f9866d64c357c475d370595d937f'], 'worker_id': ['c1018a31c3272ce74964a3280069f62f314a1a58']}\n"
     ]
    }
   ],
   "source": [
    "for k, v in qas.items():\n",
    "    print(f\"{k}, len {len(v)}\")\n",
    "\n",
    "print(qas[\"answers\"][0].keys())\n",
    "# print(qas[\"answers\"][0].keys())\n",
    "for q, a in zip(qas['question'], qas['answers']):\n",
    "    print(\"Q:\", q)\n",
    "    print(\"A:\", a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
